---
title: "üßôüèª‚Äç‚ôÇÔ∏è Resume Wizard"
output:
  html_document:
    theme: sandstone
    css: style.css
    highlight: zenburn
    df_print: paged
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
<h5 style="text-align: center;">Ardian the Great</h5>
<h5 style="text-align: center;">October 4th, 2023</h5>
<h3 style="text-align: center;">Resume Ranking & Clustering</h3>
---

# Introduction
## **Background**
```{r echo=FALSE}
knitr::include_graphics("paper images/background.jpg")
```

In today‚Äôs job market, finding the right candidates is tough. Companies need skilled professionals, and they need them fast. But here‚Äôs the problem: going through resumes one by one takes forever, and it‚Äôs not always effective. Glassdoor found that, on average, each of their corporate job postings attracts approximately 250 resumes.

To speed things up, many companies use something called an Applicant Tracking System (ATS). It‚Äôs like a robot that reads resumes and looks for specific words. It‚Äôs good at some things, but not everything.

ATS has a flaw. It only likes resumes that look a certain way. If your resume doesn‚Äôt fit the mold, it might get ignored. That‚Äôs not fair to talented people who don‚Äôt have traditional resumes. Resume Wizard is different. Instead of parsing resumes, I look at what‚Äôs inside. I check how well a resume matches what a company wants. It‚Äôs like finding a puzzle piece that fits, no matter its shape.

I want to make hiring smarter and faster. With my project, companies can find the best candidates, even if their resumes are a bit different. It‚Äôs a bridge between robots and real people, making hiring better for everyone. We believe this will help companies succeed in today‚Äôs competitive job market.

## **Project Idea**
Homework Truthweaver is an innovative web application designed to tackle the challenges of identifying plagiarism and dishonesty in academic work. Inspired by a personal experience where a ChatGPT-generated homework was rejected while other instances of copying and paraphrasing went unnoticed, this project aims to bring fairness and accuracy to the evaluation of academic assignments.

```{r echo=FALSE}
knitr::include_graphics("paper images/project-idea.jpg")
```

## **Methodologies**
### Cosine Similarity
Cosine similarity is a metric used to measure the similarity between two vectors. In the context of text analysis, these vectors often represent the term frequency of words in documents. The cosine of the angle between the vectors is computed, resulting in a similarity score. A score of 1 indicates identical vectors, while 0 implies no similarity.

```{r echo=FALSE, fig.align='center', out.width='75%'}
knitr::include_graphics("paper images/cosine-similarity.png")
```

### TF-IDF Weighting
TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is calculated by multiplying the term frequency (TF) with the inverse document frequency (IDF). TF measures how often a word appears in a document, while IDF assesses the rarity of a word across the entire corpus. TF-IDF is valuable in text analysis as it emphasizes words that are both frequent in a document and unique across the corpus, providing a more nuanced understanding of document content. TF-IDF weighting enhances cosine similarity by considering the importance of words in a document and across the entire corpus. Unlike regular cosine similarity, TF-IDF diminishes the impact of common words that appear frequently but are less informative. By highlighting unique and relevant terms, TF-IDF provides a more refined measure of document similarity, improving the accuracy of text analysis.

```{r echo=FALSE, fig.align='center', out.width='75%'}
knitr::include_graphics("paper images/tf-idf.jpg")
```

### N-Gram
N-grams are contiguous sequences of n items from a given sample of text or speech. In the context of text analysis, these items are often words. Unigrams (1-grams) represent single words, bigrams (2-grams) are pairs of consecutive words, and trigrams (3-grams) consist of three-words sequences, and so on. N-grams capture local patterns and relationships between words, allowing for a more comprehensive analysis of language structure.
 
```{r echo=FALSE, fig.align='center', out.width='75%'}
knitr::include_graphics("paper images/n-gram.png")
```

## **Project Output**
The output of this project is a Shiny Apps based Web App that can be used by anyone for analyzing document/homework similarities. And the goal is to identify instances of work dishonesty in homework grading using cosine similarity with TF-IDF weighting.

## **Impact**
### Academic Integrity Boost
Homework Truthweaver enhances academic integrity by employing advanced text analysis. Its implementation of cosine similarity with TF-IDF weighting identifies subtle patterns indicative of work dishonesty, surpassing traditional methods.

### Fair and Consistent Grading
The Analyzer promotes fairness and consistency in grading by uncovering nuanced cheating patterns. This ensures an equitable evaluation process, addressing potential disparities in treatment among students.

### Motivation for Excellence
The presence of the Analyzer serves as a motivator for students to produce original work, fostering a culture of academic excellence and deterring plagiarism.

### Improved Assessment Practices
Insights from the Analyzer‚Äôs visualization of similarity distributions empower educators to refine assessment practices continuously, adapting teaching methods and implementing proactive measures against academic dishonesty.

### Future Research Opportunities
The project opens avenues for future research on cheating patterns and academic integrity, contributing to broader scholarly discourse in educational settings.

### User-Empowered Education
The Analyzer provides educators with a user-friendly tool that not only identifies potential cheating cases but also offers valuable insights to inform instructional strategies, cultivating a culture of honesty and diligence.

## **Work Flow**
### Input:
The inputs will be homework PDFs and red flag threshold (ranging from 0% to 100%)

### Data Preprocessing:
The app converts these PDF files into raw text. Finally, the app creates a data frame containing the raw text of the pdf and the file names as row names.

### Text Preprocessing:
The text processing involves converting all text to lowercase, removing stop-words, punctuation marks, and applying text stemming to reduce words to their base forms. Additionally, extra spaces are eliminated as part of the preprocessing steps.

### Cosine Similarity with TF-IDF weighting Calculation:
The process includes converting texts into corpus, creating a Document-Term Matrix with TF-IDF weighting, and calculating the cosine similarity matrix.

### Similarity Ranking and Flagging:
With the calculated similarities, I then create two rankings, pair similarity ranking and individual homework similarity ranking. And, I also flag the pairs and individual homeworks, pairs and individual homeworks that has similarity equals or higher than red flag threshold will be considered red flags.

### Analytics:
With all the process that‚Äôs been done previously, the app then displays visualizations and tables, which are similarity distribution density, pair similarity ranking bar chart and table, and individual homework similarity bar chart and table.

### Pair Analysis:
With all the process that‚Äôs been done previously, the app new feature called Pair Analysis. With this feature, the user can input two documents. When one document is input, the user can see the work‚Äôs status (red flag or green flag), following with the similar works that exist. When two documents are input, the user can see the similarity and flag status of the pair (red flag or green flag), the web-app will also show n gram input that is gonna be set to one gram on default, then tokenize the texts of the input documents with the input n gram, and then displays 3 following word clouds: 1st document‚Äôs unique words; intersect words; and 2nd document‚Äôs unique words.

# Back-end Functions
## **Used Libraries**
```{r message=FALSE}
library(dplyr)
library(purrr)
library(tm)
library(NLP)
library(proxy)
library(NLP)
library(pdftools)
library(data.table)
library(tools)
library(tidytext)
library(textclean)
library(tokenizers)
library(tibble)
library(stringr)
library(wordcloud)
library(RColorBrewer)
```

## **Folder to Table Converter Function**
The `folder_to_table()` function serves the purpose of transforming a folder of PDF documents into a structured table, represented as a data frame.

### Function Input:
- ***folder_path***: The path to the PDFs folder

### Function Workflow:
1. Extract the raw text
2. Extract the file name
3. Create the data frame

### Function Output:
The output is a data frame with the row names representing the file name and column text representing the raw extracted text.

### Function code:
```{r}
folder_to_table <- function(folder_path) {
  # Function to convert PDF to text
  convert_pdf_to_text <- function(pdf_path) {
    pdf_text_content <- pdf_text(pdf_path)
    
    extracted_text <- list()
    
    for (page in seq_along(pdf_text_content)) {
      text <- pdf_text_content[[page]]
      extracted_text[[page]] <- text
    }
    
    all_text <- paste(extracted_text, collapse = "\n")
  }
  
  # Function to get file name
  get_file_name <- function(file_path) {
    file_path_sans_ext(basename(file_path))
  }
  
  # Getting PDF files from the specified folder
  pdf_files <- list.files(folder_path, pattern = ".pdf", full.names = TRUE)
  
  # Converting PDFs to text
  pdf_texts <- lapply(pdf_files, convert_pdf_to_text)
  
  # Creating a data frame for the extracted texts
  table_data <- data.frame(
    text = unlist(pdf_texts)
  )
  
  # Renaming the data frame rows with the file names
  rownames(table_data) <- get_file_name(pdf_files)
  
  return(table_data)
}
```

### Function Test:
```{r}
df <- folder_to_table("dataset")

df %>% head()
```

## **Text Cleaner Function**
The `clean_text()` function is designed to clean text data.

### Function Input:
- ***text***: Raw text input.
- ***as_corpus***: If TRUE, the function will return the text in the form of a corpus.
- *Other conditional statements*: These are conditional statements specifying transformations to be applied to the text.

### Function Workflow:
1. Transform the input text into a text corpus
2. Convert all text to lowercase
3. Remove stop-words
4. Remove punctuation marks
5. Apply text stemming to reduce words to their base forms
6. Eliminate extra spaces
7. Return the cleaned text corpus

### Function Output:
The output is the cleaned text in the form of a character or corpus based on the `as_corpus` argument.

### Function Code:
```{r}
clean_text <- function(text, as.corpus = T, lower = T, rm.number = T, rm.stopwords_english = T, rm.stopwords_bahasa = T, rm.punctuation = T, stem = T, rm.whitespace = T){
  text_corpus <- text %>% VectorSource() %>% VCorpus()
  
  # Lowercasing
  if (lower){
    text_corpus <- tm_map(x = text_corpus,
                          FUN = content_transformer(tolower))
  }
  # Removing numbers
  if (rm.number){
    text_corpus <- tm_map(x = text_corpus,
                          FUN = removeNumbers)
  }
  # Removing english stop words
  if (rm.stopwords_english){
    list_stop_words_english <- readLines("stop-words_english.txt", warn = FALSE, encoding = "UTF-8")
    
    text_corpus <- tm_map(x = text_corpus,
                          FUN = removeWords,
                          list_stop_words_english)
  }
  # Removing bahasa stop words
  if (rm.stopwords_bahasa){
    list_stop_words_bahasa <- readLines("stop-words_bahasa.txt", warn = FALSE, encoding = "UTF-8")
    
    text_corpus <- tm_map(x = text_corpus,
                          FUN = removeWords,
                          list_stop_words_bahasa)
  }
  # Removing punctuation
  if (rm.punctuation){
    text_corpus <- tm_map(x = text_corpus,
                          FUN = removePunctuation)
  }
  # Reducing words to their base form
  if (stem){
    text_corpus <- tm_map(x = text_corpus,
                          FUN = stemDocument)
  }
  # Removing white/blank spaces
  if (rm.whitespace){
    text_corpus <- tm_map(x = text_corpus,
                          FUN = stripWhitespace)
  }
  
  # Returning the text as or not as corpus
  if (as.corpus){
    return(text_corpus)
  }
  else(
    return(sapply(text_corpus, as.character))
  )
}
```

> You may think "Why you gotta put if else statements for each transformation?" Well it's because this function is taken directly from my personal package.

### Function Test:
```{r}
df$text <- clean_text(df$text, F)

df %>% head()
```

## **Word Cloud Function**
The `get_wordcloud()` function is created to generate a word cloud that emphasizes words for specific analysis.

### Function Input:
- ***words***: A string of words.
- ***palette***: The color palette for the word clouds.
- ***n_gram***: The n gram.

### Function Workflow:
1. Tokenize the words
2. Create a data frame containing the frequency of each word
3. Generate a word cloud plot

### Function Output:
The output is a word cloud plot representing the input text/words.

### Function Code:
```{r}
get_wordcloud <- function(words, palette, n_gram = 1){
  # Tokenizing the input string into individual words
  words_list <- tokenize_ngrams(words, n = n_gram) %>% unlist()
  upper_scale <- ifelse(n_gram > 1, 2, 3)
  
  # Creating a data frame of matching words and count their occurrences.
  words <- data.frame(word = words_list) %>%
    count(word, sort = TRUE)
  
  # Generating a word cloud with specified settings, scaling word size by frequency
  words %>%
    with(
      wordcloud(
        words = word,
        random.order = FALSE,
        color = brewer.pal(9, palette),
        min.freq = 1,
        scale = c(upper_scale, 1),
        rot.per = 0,
        freq = n,
      )
    )
}
```

### Function Test: {.tabset}

#### Unigram
```{r warning=FALSE}
get_wordcloud(df[1, 1], "Greens", 1)
```

#### Bigram
```{r warning=FALSE}
get_wordcloud(df[1, 1], "Greens", 2)
```

#### Trigram
```{r warning=FALSE}
get_wordcloud(df[1, 1], "Greens", 3)
```

## **Resume Ranking Function**
The `rank_resumes()` function is designed to rank resumes based on their similarity to desired criteria. This function takes two inputs: the ideal criteria and a data frame containing resumes. It then produces a ranked list of resumes, with those most closely matching the criteria receiving higher ranks.

### Function Input:
- ***ideal_criteria***: This is the set of criteria or keywords that define the ideal candidate. It serves as a reference for ranking resumes.
- ***resume_df***: This data frame should contain the resumes to be ranked, with each resume represented as text in the `text` column.

### Function Workflow:
1. Clean the ideal criteria and resumes texts
2. Tokenize the cleaned ideal criteria
3. Calculate how many matching words between ideal criteria and the resume text
4. Score resumes based on matched word count
5. Sort Resumes by Score
6. Return the resume ranking as a data frame

### Function Output:
The output is a data frame that includes the following columns:

- **Resume**: The names or identifiers of the resumes.
- **Matching_Words**: A string containing the words that matched between the resume and the ideal criteria, separated by spaces.
- **Matched_Ideal**: A ratio indicating the proportion of matched words relative to the total words in the ideal criteria.

### Function Code:
```{r}
rank_resumes <- function(ideal_criteria, resumes_df) {
  # Cleaning the ideal criteria and resume text
  clean_ideal_criteria <- clean_text(ideal_criteria, FALSE)
  resumes_df$text <- clean_text(resumes_df$text, FALSE)
  
  # Tokenizing the cleaned ideal criteria into unique words
  unlist_ideal_criteria <- unlist(str_split(clean_ideal_criteria, " ")) %>% unique()
  
  # Checking if the resume data frame is empty
  if (nrow(resume_df) == 0) {
    stop("There's no PDF/resume.")
  }
  
  # Calculating word count, matched ideal words, and rank resumes
  ranking_df <- resume_df %>%
    mutate(
      Word_Count = map_int(tolower(text), ~ length(intersect(unlist_ideal_criteria, unlist(str_split(., "\\W+"))))),
      Matching_Words = map_chr(tolower(text), ~ paste(intersect(unlist_ideal_criteria, unlist(str_split(., "\\W+"))), collapse = " ")),
      Matching_Percentage = round(Word_Count / length(unlist_ideal_criteria), 2)
    ) %>%
    arrange(desc(Word_Count)) %>%
    mutate(Rank = row_number()) %>%
    select(-Word_Count) %>%
    column_to_rownames(var = "file_name")
  
  return(ranking_df %>% select(text))
}
```

### Function Test:
```{r}
similarity_matrix <- get_cosine_similarity_matrix(df)

as.data.frame(similarity_matrix)[1:10, 1:10]
```

## **Resume Clustering Function**
The `cluster_resume()` function is made to cluster the ranked resumes based on the matching criteria.

### Function Input:
- ***ranking_df***: The resumes ranking with the matching criteria.
- ***k***: The number of clusters/segments.

### Function Workflow:
1. Create document term matrix for the resumes ranking data frame
2. Apply K-means clustering with ***k*** centers

### Function Output:
The output is a k-means cluster model of the resumes.

### Function Code:
```{r}
cluster_resumes <- function(ranking_df, k){
  # Creating document term matrix of ranking_df
  dtm <- DocumentTermMatrix(ranking_df$Matching_Words)
    
  # Converting the document term matrix into a data frame
  df <- dtm %>%
    as.matrix() %>% 
    as.data.frame()
    
  # Replacing the row names with the resume file names
  rownames(df) <- rownames(ranking_df)
  
  # Applying K-means Clustering algorithm 
  clusters <- kmeans(x = df,
                     centers = k)
  
  return(clusters)
}
```

### Function Test:
```{r}
red_flag_threshold <- 0.1

pair_similarity_ranking <- get_pair_similarity_ranking(similarity_matrix, red_flag_threshold)

pair_similarity_ranking
```

## **Homework Similarity Ranking Function**
The `get_homework_similarity_ranking()` function is developed to rank every homework based on their highest calculated similarity.

### Function Input:
- ***similarity_matrix***: A matrix resulting from the `get_cosine_similarity_matrix()` function.
- ***red_flag_threshold***: A percentage number used to flag homework; homework with the highest similarity equal to or above the threshold will be flagged as "RED."

### Function Workflow:
1. Convert the similarity matrix into a long-format data frame
2. Sort the data frame by similarity values in descending order
3. Remove rows where the pair consists of the same document
4. Group by each document
5. Keep only the row with the highest similarity within each homework
6. Flag each homework

### Function Output:
A data frame with three columns: Document; Document_2; Highest_Similarity; Highest_Similarity_Formatted; and Flag.

### Function Code:
```{r}
get_clusters_words <- function(clusters, ranking_df){
  # Creating an empty array for cluster words
  words_list <- c()
  
  # Storing each cluster's words inside the array
  for (center in 1:nrow(clusters$centers)){
    cluster_assignments <- clusters$cluster
    
    cluster_index <- which(cluster_assignments == center)
    
    cluster_data <- ranking_df[cluster_index, ]
    
    cluster_words <- paste(cluster_data, collapse = " ")
    
    words_list[center] <- cluster_words
  }
  
  return (words_list)
}
```

### Function Test:
```{r}
homework_similarity_ranking <- get_homework_similarity_ranking(similarity_matrix, red_flag_threshold)

homework_similarity_ranking
```

## **Similar Works Function**
The `get_similar_works()` function is developed to retrieve similar works for the input documents.

### Function Input:
- ***pair_similarity_ranking***: A ranking resulting from the `get_pair_similarity_ranking()` function.
- ***document***: The document name.

### Function Workflow:
1. Filter the `pair_similarity_ranking` row containing the input document
2. Filter the row where it‚Äôs flagged as "RED"
3. Return the similar works data frame

### Function Output:
The output is a data frame containing documents that are similar to the input document. Consists of two columns, Document (document name) and Similarity.

### Function Code:
```{r}
get_similar_works <- function(pair_similarity_ranking, document) {
  
  similar_works <- pair_similarity_ranking %>% 
    # Filter pair_similarity_ranking where it's input_homework and where it's red flag
    filter(
      document == Document_1 | document == Document_2,
      Flag == "RED"
    ) %>% 
    
    # Modifying the Document column by removing input_homework and "&" from the Documents column
    mutate(
      Document = gsub(paste(c(document, "&"), collapse = "|"), "", Documents),
      Document = trimws(Document)
    ) %>% 
    
    # Selecting and reformatting the similarity value
    select(Document, Similarity_Formatted) %>% 
    rename(Similarity = Similarity_Formatted)
  
  # Return the resulting data frame
  return(similar_works)
}
```

### Function Test:
```{r}
similar_works_19164410 <- get_similar_works(pair_similarity_ranking, "TI100_Asteroid_6364656667")

similar_works_19164410
```

## **Intersect and Distinct Words Function**
The `get_intersect_and_distinct_words()` function is developed to extract intersect and distinct tokens between two texts.

### Function Input:
- ***text_1***: A string of words/tokens for the 1st document.
- ***text_2***: A string of words/tokens for the 2nd document.

### Function Workflow:
1. Tokenize both texts
2. Combine both tokens
3. Find intersect tokens
4. Create a map of the 1st document‚Äôs distinct tokens, intersect tokens, and 2nd document‚Äôs distinct tokens
5. Return the words/tokens map

### Function Output:
The output is a map of tokens, which includes:

- 1st document‚Äôs distinct tokens
- Intersect tokens
- 2nd document‚Äôs distinct tokens.

### Function Code:
```{r}
get_intersect_and_distinct_words <- function(text_1, text_2) {
  
  # Tokenizing both texts
  text_1_tokens <- unlist(strsplit(tolower(text_1), "\\s+"))
  text_2_tokens <- unlist(strsplit(tolower(text_2), "\\s+"))
  
  # Combining tokens
  combined_tokens <- c(text_1_tokens, text_2_tokens)
  
  # Finding intersect words between both texts
  intersect_tokens <- intersect(text_1_tokens, text_2_tokens)
  
  # Creating a list containing distinct words for document_1, intersecting words, and distinct words for document_2
  words <- list(
    "document_1_distinct" = text_1_tokens[!text_1_tokens %in% intersect_tokens] %>% paste(collapse = " ") %>% as.character(),
    "intersect" = combined_tokens[combined_tokens %in% intersect_tokens] %>% paste(collapse = " ") %>% as.character(),
    "document_2_distinct" = text_2_tokens[!text_2_tokens %in% intersect_tokens] %>% paste(collapse = " ") %>% as.character()
  )
  
  return(words)
}
```

### Function Test:
```{r}
text_1 <- df["TI100_Asteroid_6364656667", "text"]
text_2 <- df["TI100_Meteor_5354555657", "text"]

get_intersect_and_distinct_words(text_1, text_2)
```

# Visualizations

## **Pair Similarity Density Chart**
This chart visualizes the distribution of similarity scores between pairs of homeworks.
```{r}
color <- ifelse(pair_similarity_ranking$Similarity %>% median() >= red_flag_threshold, "red", "green")
                
similarity_distribution_density_chart <- ggplot(pair_similarity_ranking, aes(x = Similarity * 100)) +
  geom_density(fill = color, color = color, alpha = 0.4) +
  labs(x = "Similarity (%)",
       y = NULL,
       title = "Pair Similarity Density") +
  theme_minimal() +
  scale_x_continuous(labels = percent_format(scale = 1), limits = c(0, 100)) +
  geom_vline(xintercept = red_flag_threshold * 100, color = "black")

similarity_distribution_density_chart
```

## **Homework Similarity Density Chart**
This chart visualizes the distribution of similarity scores of individual homeworks.
```{r}
color <- ifelse(homework_similarity_ranking$Highest_Similarity %>% median() >= red_flag_threshold, "red", "green")

similarity_distribution_density_chart <- ggplot(homework_similarity_ranking, aes(x = Highest_Similarity * 100)) +
  geom_density(fill = color, color = color, alpha = 0.4) +
  labs(x = "Similarity (%)",
       y = NULL,
       title = "Homework Distribution Density") +
  theme_minimal() +
  scale_x_continuous(labels = percent_format(scale = 1), limits = c(0, 100)) +
  geom_vline(xintercept = red_flag_threshold * 100, color = "black")

similarity_distribution_density_chart
```

## **Top 12 Pairs of Homework Bar Chart**
This bar chart displays the top 12 pairs of homeworks with the highest similarity scores.
```{r}
top_12_pairs_bar_chart <- ggplot(pair_similarity_ranking[1:12, ], aes(y = reorder(Documents, Similarity), x = Similarity * 100, fill = Similarity, text = paste0(Documents, "\n", Similarity_Formatted, " similar"))) +
  geom_bar(stat = "identity", show.legend = F) +
  labs(title = "Top 12 Similar Pairs of Homeworks", y = NULL, x = "Similarity (%)") +
  scale_x_continuous(labels = percent_format(scale = 1)) +
  scale_fill_gradient(low = "#790000", high = "#FF0D0D") +
  theme_minimal() +
  theme(text = element_text(size = 8))
        
top_12_pairs_bar_chart <- top_12_pairs_bar_chart %>% ggplotly(tooltip = "text")

top_12_pairs_bar_chart
```

## **Top 12 Homeworks Bar Chart**
This bar chart displays the top 12 individual homeworks with the highest similarity to others.
```{r}
top_12_homeworks_bar_chart <- ggplot(homework_similarity_ranking[1:12, ], aes(y = reorder(Document, Highest_Similarity), x = Highest_Similarity * 100, fill = Highest_Similarity, text = paste(Highest_Similarity_Formatted, "similar to", Other_Document))) +
          geom_bar(stat = "identity", show.legend = F) +
          labs(title = "Top 12 High Similarity Homeworks", y = NULL, x = "Similarity (%)") +
          scale_x_continuous(labels = percent_format(scale = 1)) +
          scale_fill_gradient(low = "#790000", high = "#FF0D0D") +
          theme_minimal() +
          theme(text = element_text(size = 9))
        
top_12_homeworks_bar_chart <- top_12_homeworks_bar_chart %>% ggplotly(tooltip = "text")

top_12_homeworks_bar_chart
```

## **Pair Word Clouds** {.tabset}
These word clouds shows the distribution of intersect and distinct words between two documents.
```{r}
document_1 <- "TI100_Asteroid_6364656667"
document_2 <- "TI100_Meteor_5354555657"

text_1 <- df[document_1, "text"]
text_2 <- df[document_2, "text"]
```

### 1st document's unique words
```{r warning=FALSE}
get_intersect_and_distinct_words(text_1, text_2)$document_1_distinct %>% get_wordcloud("Greens", 2)
```

### Intersect words
```{r warning=FALSE}
get_intersect_and_distinct_words(text_1, text_2)$intersect %>% get_wordcloud("Reds", 2)
```

### 2nd document's unique words
```{r warning=FALSE}
get_intersect_and_distinct_words(text_1, text_2)$document_2_distinct %>% get_wordcloud("Greens", 2)
```

# Shiny Web-app
Here is the link to the web-app: https://ardian.shinyapps.io/Homework-Truthweaver/

## **Overview** {.tabset}
### File Input
```{r echo=FALSE}
knitr::include_graphics("paper images/overview-file-input.png")
```

### Analytics
```{r echo=FALSE}
knitr::include_graphics("paper images/overview-analytics.png")
```

### Analytics (contd)
```{r echo=FALSE}
knitr::include_graphics("paper images/overview-analytics-2.png")
```

### Pair Analysis
```{r echo=FALSE}
knitr::include_graphics("paper images/overview-pair-analysis.png")
```

### Pair Analysis (contd)
```{r echo=FALSE}
knitr::include_graphics("paper images/overview-pair-analysis-2.png")
```

## **User Experience Flow**
1. The user uploads works in PDF format.
2. The user set the red flag threshold (pairs and works with similarity equals or more than the threshold will be count as red flag pairs or works).
3. The user clicks ‚Äúconjure‚Äù and the web-app displays 2 rows, Analytics and Pair Analysis.
4. In Analytics, the user can see the similarity distribution density, pair similarity ranking, and individual work similarity ranking.
5. In Pair Analysis, the user can input two documents. When one document is input, the user can see the work‚Äôs status (red flag or green flag), following with the similar works that exist. When two documents are input, the user can see the similarity and status of the pair (red flag or green flag) , the web-app will also displays 3 following word clouds: 1st document‚Äôs unique words; intersect words; and 2nd document‚Äôs unique words.

## **Limiation & Notes**
1. The documents input must be in PDF format.
2. Only texts will be extracted, which means images will be ignored.
